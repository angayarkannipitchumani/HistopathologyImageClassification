{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimage_files=[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        image_files.extend([os.path.join(dirname,filename)])\nprint(len(image_files))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(image_files)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom glob import glob\nimport itertools\nimport fnmatch\nimport random\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport cv2\n\nimport sklearn\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport keras\nfrom keras import backend as K\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, model_from_json\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\nfrom keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from glob import glob\ni_=0\n#imagePatches = glob('../input/IDC_regular_ps50_idx5/**/*.png', recursive=True)\nfor filename in range(25):\n    print(image_files[filename])\n    image = cv2.imread(image_files[filename])\n    im = cv2.resize(image, (50, 50)) \n    plt.subplot(5, 5, filename+1) #.set_title(l)\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)); plt.axis('on')\n    i_ += 1\n    \n# Plot Multiple Images\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef randomImages(a):\n    r = random.sample(a, 4)\n    plt.figure(figsize=(16,16))\n    plt.subplot(131)\n    plt.imshow(cv2.imread(r[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(r[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(r[2])); \nrandomImages(image_files)\n\n   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patternZero = '*class0.png'\npatternOne = '*class1.png'\nclassZero = fnmatch.filter(image_files, patternZero)\nclassOne = fnmatch.filter(image_files, patternOne)\nprint(\"IDC(-)\\n\\n\",classZero[0:10],'\\n')\nprint(\"IDC(+)\\n\\n\",classOne[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom matplotlib import pyplot as plt\n\nimg = cv2.imread('/kaggle/input/breast-histopathology-images/12935/0/12935_idx5_x451_y1751_class0.png',0)\n\nhist,bins = np.histogram(img.flatten(),256,[0,256])\n\ncdf = hist.cumsum()\ncdf_normalized = cdf * hist.max()/ cdf.max()\nplt.subplot(121)\nplt.plot(cdf_normalized, color = 'b')\nplt.hist(img.flatten(),256,[0,256], color = 'r')\nplt.xlim([0,256])\nplt.legend(('cdf','histogram'), loc = 'upper left')\n\nplt.show()\ncdf_m = np.ma.masked_equal(cdf,0)\ncdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min())\ncdf = np.ma.filled(cdf_m,0).astype('uint8')\nimg2 = cdf[img]\nplt.subplot(122),plt.imshow(img2),plt.title('FHE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\n\nimg = cv2.imread('/kaggle/input/breast-histopathology-images/12935/0/12935_idx5_x451_y1751_class0.png',0)\n\n# create a CLAHE object (Arguments are optional).\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\ncl1 = clahe.apply(img)\n\ncv2.imwrite('clahe_2.jpg',cl1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\n\nimg = cv.imread('/kaggle/input/breast-histopathology-images/12935/0/12935_idx5_x451_y1751_class0.png',0) # loads in grayscale\n\nalpha = 1\nbeta = 0\nres = cv.multiply(img, alpha)\nres = cv.add(res, beta)\nres = cv.convertScaleAbs(img, alpha = alpha, beta = beta)\nplt.subplot(122),plt.imshow(res),plt.title('FHE')\n    #cv2.imwrite(image_files[filename], blur)\nplt.xticks([]), plt.yticks([])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('/kaggle/input/breast-histopathology-images/12935/0/12935_idx5_x451_y1751_class0.png',0)\nequ = cv2.equalizeHist(img)\nres = np.hstack((img,equ)) #stacking images side-by-side\ncv2.imwrite('res.png',res)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist,bins = np.histogram(res.flatten(),256,[0,256])\n\ncdf = hist.cumsum()\ncdf_normalized = cdf * hist.max()/ cdf.max()\nplt.subplot(121)\nplt.plot(cdf_normalized, color = 'b')\nplt.hist(img.flatten(),256,[0,256], color = 'r')\nplt.xlim([0,256])\nplt.legend(('cdf','histogram'), loc = 'upper left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport cv2\n\nimg = cv2.imread('/kaggle/input/breast-histopathology-images/12935/0/12935_idx5_x451_y1751_class0.png',0)\n\n# create a CLAHE object (Arguments are optional).\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\ncl1 = clahe.apply(img)\n\ncv2.imwrite('clahe_2.jpg',cl1)\nplt.subplot(122),plt.imshow(cl1),plt.title('FHE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply Gaussian Filter\ni=0\ngrayimg=[]\nfor filename in range(25):\n    #print(image_files[filename])\n    image = cv2.imread(image_files[filename])\n    #img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    #blur = cv2.GaussianBlur( img_gray,(5,5),0)\n    median = cv2.medianBlur(image,5)\n    plt.subplot(121),plt.imshow(img_gray),plt.title('Original')\n    plt.xticks([]), plt.yticks([])\n    plt.subplot(122),plt.imshow(img_gray),plt.title('FHE')\n    #cv2.imwrite(image_files[filename], blur)\n    plt.xticks([]), plt.yticks([])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Segmentation using threshold\nfrom pylab import *\nfrom PIL import *\n\n# create a new figure\nfigure()\ngray()\n\nfor filename in range(25):\n    \n    original = cv2.imread(image_files[filename])\n    grayimg = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n    median = cv2.medianBlur(img_gray,5)\n    \n    ret, thresh = cv2.threshold(median , 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Displaying segmented images\n  # show contours with origin upper left corner\n    contour(thresh, origin='image')\n    axis('equal')\n    axis('off')\n\n\n    figure()\n\n\n    hist(thresh.flatten(), 128)\n\n    show()\n    \n\n\n    figure()\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nbase_path = '../input/breast-histopathology-images/IDC_regular_ps50_idx5/'\n# We will save the base path to the patient folders, so that we can easily loop over them\npatient_ids = listdir(base_path)\n\nclass_0_total = 0\nclass_1_total = 0\nfrom pprint import pprint\nfor patient_id in patient_ids:\n    class_0_files = listdir(base_path + patient_id + '/0')\n    class_1_files = listdir(base_path + patient_id + '/1')\n\n    class_0_total += len(class_0_files)\n    class_1_total += len(class_1_files) \n\ntotal_images = class_0_total + class_1_total\n    \nprint(f'Number of patches in Class 0: {class_0_total}')\nprint(f'Number of patches in Class 1: {class_1_total}')\nprint(f'Total number of patches: {total_images}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# note that we loop through the classes after looping through the \n# patient ids so that we avoid splitting our data into [all class 0 then all class 1]\n# Segmentation using threshold\nfrom pylab import *\nfrom PIL import *\nfrom tqdm.notebook import tqdm\ntqdm().pandas();\n\n\n\n# create a new figure\ncolumns = [\"patient_id\",'x','y',\"target\",\"path\"]\ndata_rows = []\ni = 0\niss = 0\nisss = 0\nfigure()\ngray()\n\nfor filename in range(25):\n    for patient_id in tqdm(patient_ids):\n        for c in [0,1]:\n            class_path = base_path + patient_id + '/' + str(c) + '/'\n            imgs = listdir(class_path)\n        \n        # Extracting Image Paths\n            img_paths = [class_path + img + '/' for img in imgs]\n        \n        # Extracting Image Coordinates\n            img_coords = [img.split('_',4)[2:4] for img in imgs]\n            x_coords = [int(coords[0][1:]) for coords in img_coords]\n            y_coords = [int(coords[1][1:]) for coords in img_coords]\n\n        for (path,x,y) in zip(img_paths,x_coords,y_coords):\n            values = [patient_id,x,y,c,path]\n            data_rows.append({k:v for (k,v) in zip(columns,values)})\n# We create a new dataframe using the list of dicts that we generated above\n        data = pd.DataFrame(data_rows)\n        print(f'Shape of Dataframe: {data.shape}')\n        data.head()\n    \n    original = cv2.imread(image_files[filename])\n    grayimg = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n    median = cv2.medianBlur(img_gray,5)\n    \n    ret, thresh = cv2.threshold(median , 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Displaying segmented images\n  # show contours with origin upper left corner\n    contour(thresh, origin='image')\n    axis('equal')\n    axis('off')\n\n\n    figure()\n\n\n    hist(thresh.flatten(), 128)\n\n    show()\n    \n\n\n    figure()\n   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [\"patient_id\",'x','y',\"target\",\"path\"]\ndata_rows = []\ni = 0\niss = 0\nisss = 0\n\n# note that we loop through the classes after looping through the \n# patient ids so that we avoid splitting our data into [all class 0 then all class 1]\nfor patient_id in tqdm(patient_ids):\n    for c in [0,1]:\n        class_path = base_path + patient_id + '/' + str(c) + '/'\n        imgs = listdir(class_path)\n        \n        # Extracting Image Paths\n        img_paths = [class_path + img + '/' for img in imgs]\n        \n        # Extracting Image Coordinates\n        img_coords = [img.split('_',4)[2:4] for img in imgs]\n        x_coords = [int(coords[0][1:]) for coords in img_coords]\n        y_coords = [int(coords[1][1:]) for coords in img_coords]\n\n        for (path,x,y) in zip(img_paths,x_coords,y_coords):\n            values = [patient_id,x,y,c,path]\n            data_rows.append({k:v for (k,v) in zip(columns,values)})\n# We create a new dataframe using the list of dicts that we generated above\ndata = pd.DataFrame(data_rows)\nprint(f'Shape of Dataframe: {data.shape}')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return random sample indexes that are cancer positive and cancer negative of size 50\n# replace = False means that no duplication is allowed\nfrom pylab import *\nfrom PIL import *\nfrom skimage.feature import greycomatrix, greycoprops\nfrom multiprocessing import Pool\nfrom tqdm import tqdm_notebook as tqdm\nfrom skimage import io\n\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfigure()\ngray()\n\n\n\n#patient_ids = np.random.choice( data.patient_id.unique(), size=n_imgs, replace=False)\n#fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\n\n\nn_rows = 10\nn_cols = 10\n\n#Cancerous Patches\n#fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\nn_rows = 5\nn_cols = 3\nn_imgs = n_rows*n_cols\ncolors = ['pink', 'purple']\nfig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\n#fig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\npatient_ids = np.random.choice( data.patient_id.unique(), size=n_imgs, replace=False)\npositive_tissue = np.random.choice(data[data.target==1].index.values, size=100, replace=False)\nnegative_tissue = np.random.choice(data[data.target==0].index.values, size=100, replace=False)\n\nfor row in tqdm(range(n_rows)):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indexes\n        idx = positive_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        patient_id = patient_ids[col + n_cols*row]\n        patient_df = get_patient_df(patient_id)\n        \n        ax[row,col].scatter(patient_df.x.values, \\\n                            patient_df.y.values, \\\n                            c=patient_df.target.values,\\\n                            cmap=ListedColormap(colors), s=20)\n        ax[row,col].set_title(\"patient \" + patient_id)\n\n       # ax[row,col].imshow(img[:,:,:])\n       # ax[row,col].grid(False)\n        \nfor row in tqdm(range(n_rows)):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indexes\n        idx = positive_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        grayimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        median = cv2.medianBlur(grayimg,5)\n        \n        contour(median, origin='image')\n        axis('equal')\n        axis('off')\n        figure()\n        hist(median.flatten(), 128)\n        show()\n        figure()\n        img_array = (median.flatten())\n\n        img_array  = img_array.reshape(-1, 1).T\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_patient_df(patient_id):\n    return data.loc[data['patient_id']== patient_id,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Non Cancerous Patches\")\n\n\n#Non Cancerous Patches\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nn_cols = 3\nn_imgs = n_rows*n_cols\ncolors = ['pink', 'purple']\n\nfig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\npatient_ids = np.random.choice( data.patient_id.unique(), size=n_imgs, replace=False)\n#fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\nfor row in tqdm(range(n_rows)):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indices\n        idx = negative_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        img = cv2.medianBlur(img,5)\n        #ax[row,col].imshow(img[:,:,:])\n        #ax[row,col].grid(False)\n        patient_id = patient_ids[col + n_cols*row]\n        patient_df = get_patient_df(patient_id)\n        \n        ax[row,col].scatter(patient_df.x.values, \\\n                            patient_df.y.values, \\\n                            c=patient_df.target.values,\\\n                            cmap=ListedColormap(colors), s=20)\n        ax[row,col].set_title(\"patient \" + patient_id)\n\n        \nfor row in tqdm(range(n_rows)):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indexes\n        idx = positive_tissue[col + n_cols*row]\n        grayimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        median = cv2.medianBlur(grayimg,5)\n        img = io.imread(data.loc[idx, \"path\"])\n        contour(median, origin='image')\n        patient_id = patient_ids[col + n_cols*row]\n        patient_df = get_patient_df(patient_id)\n        ax[row,col].set_title(\"patient \" + patient_id)\n        axis('equal')\n        axis('on')\n        figure()\n        hist(median.flatten(), 128)\n        show()\n        figure()\n        img_array = (median.flatten())\n\n        img_array  = img_array.reshape(-1, 1).T\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef split_data(data,train_test_percentage = 0.7,percentage_of_dataset = 0.2):\n    # get number of datapoints to split from the total dataset\n    n_points = round(data.shape[0]*0.2)\n    # retrieve the desired initial split of the data based on the specified percentage of the dataset\n    data = data[0:n_points]\n    # retrieve all the columns except the target columns and store them in X\n    X = data.loc[:,data.columns != 'target']\n    # retrieve the target column and store them in y \n    y = data.loc[:,['target']]\n    # Split the model to training and testing sets for both the features and the targets\n    X_train, X_test, y_train, y_test =  train_test_split(X, y, train_size=train_test_percentage, shuffle = False)\n    return [X_train, X_test, y_train, y_test]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = split_data(data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.color import rgb2gray\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy import ndimage\nfrom sklearn.cluster import KMeans\n\nfor i in range(10):\n    print(i)\n    img_path = X_train.path[i]\n# reading the images\n    rgb_img = io.imread(img_path)\n    gray = io.imread(img_path, as_gray = True)\n    kmeans = KMeans(n_clusters=5, random_state=0).fit(gray)\n    pic2show = kmeans.cluster_centers_[kmeans.labels_]\n    plt.subplot(121),plt.imshow(pic2show),plt.title('After KMeans')\n    plt.xticks([]), plt.yticks([])\n    plt.show()\n   # converting to grayscale\n    #gray = rgb2gray(image)\n\n# defining the sobel filters\n    \n   \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=5, random_state=0).fit(pic_n)\npic2show = kmeans.cluster_centers_[kmeans.labels_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport time\nimport tqdm\nimport random\nimport collections\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom PIL import Image\nfrom functools import partial\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm as tq\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n\n# ablumentations for easy image augmentation for input as well as output\nimport albumentations as albu\n# from albumentations import torch as AT\nplt.style.use('bmh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom PIL import Image\nfrom skimage.color import rgb2hsv\nimport os\nimg = Image.open(r'/kaggle/input/breast-histopathology-images/9176/0/9176_idx5_x1201_y801_class0.png')\nx = np.array(img)\nimshow(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = np.dstack((x,rgb2hsv(x)))\nz.shape\nvectorized = np.float32(z.reshape((-1,6)))\nvectorized.shape\nkmeans = KMeans(random_state=0, init='random', n_clusters=8)\nlabels = kmeans.fit_predict(vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nlabels.shape\npic = labels.reshape(50,50)\nf, axarr = plt.subplots(1,2,figsize=(15,15))\naxarr[0].set_xlabel('Original Image', fontsize=12)\naxarr[1].set_xlabel('Segmented Image', fontsize=12)  \naxarr[0].imshow(x)\naxarr[1].imshow(pic)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for filename in range(25):\n    img = Image.open(image_files[filename])\n    x = np.array(img)\n    imshow(x)\n    x = np.array(img)\n    z = np.dstack((x,rgb2hsv(x)))\n    z.shape\n    vectorized = np.float32(z.reshape((-1,6)))\n    kmeans = KMeans(random_state=32, init='random', n_clusters=6)\n    james = kmeans.fit_predict(vectorized)\n    pic = james.reshape(50,50)\n    f, axarr = plt.subplots(1,2,figsize=(15,15))\n    axarr[0].set_xlabel('Original Image', fontsize=12)\n    axarr[1].set_xlabel('Segmented Image', fontsize=12)  \n    axarr[0].imshow(x)\n    axarr[1].imshow(pic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Watershed\nfor filename in range(25):\n    imgs=cv2.imread('/kaggle/input/breast-histopathology-images/9176/1/9176_idx5_x2201_y1201_class1.png')\n    img = cv2.imread('/kaggle/input/breast-histopathology-images/9176/1/9176_idx5_x2201_y1201_class1.png')\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    # noise removal\n    kernel = np.ones((5,5),np.uint8)\n    closing = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations = 2) \n    # sure background area\n    sure_bg = cv2.dilate(closing,kernel,iterations=3)\n    # Finding sure foreground area\n    dist_transform = cv2.distanceTransform(closing,cv2.DIST_L2,5)\n    ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n    # Finding unknown region\n    sure_fg = np.uint8(sure_fg)\n    unknown = cv2.subtract(sure_bg,sure_fg)\n\n    # Marker labelling\n    ret, markers = cv2.connectedComponents(sure_fg)\n    # Add one to all labels so that sure background is not 0, but 1\n    markers = markers+1\n    # Now, mark the region of unknown with zero\n    markers[unknown==255] = 0\n\n    markers = cv2.watershed(img,markers)\n    img[markers == -1] = [255,0,0]\n    #plt.imshow(markers)\n    #plt.imshow(img)\n    f, axarr = plt.subplots(1,2,figsize=(15,15))\n    axarr[0].set_xlabel('Original Image', fontsize=12)\n    axarr[1].set_xlabel('Segmented Image', fontsize=12)  \n    axarr[0].imshow(imgs)\n    axarr[1].imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img=cv2.imread('/kaggle/input/breast-histopathology-images/14304/0/14304_idx5_x1151_y2201_class0.png')\n#img = cv2.imread('/kaggle/input/breast-histopathology-images/9176/1/9176_idx5_x2201_y1201_class1.png')\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    # noise removal\nkernel = np.ones((5,5),np.uint8)\nclosing = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations = 2) \n    # sure background area\nsure_bg = cv2.dilate(closing,kernel,iterations=3)\n    # Finding sure foreground area\ndist_transform = cv2.distanceTransform(closing,cv2.DIST_L2,5)\nret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n    # Finding unknown region\nsure_fg = np.uint8(sure_fg)\nunknown = cv2.subtract(sure_bg,sure_fg)\n\n    # Marker labelling\nret, markers = cv2.connectedComponents(sure_fg)\n    # Add one to all labels so that sure background is not 0, but 1\nmarkers = markers+1\n    # Now, mark the region of unknown with zero\nmarkers[unknown==255] = 0\n\nmarkers = cv2.watershed(img,markers)\nimg[markers == -1] = [255,0,0]\n#plt.imshow(markers)\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\n\ndef compute_simple_mask(image):\n  image_grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n  return cv2.threshold(image_grayscale, 127, 255, 0)[1]\n\n\nimage_path = '/kaggle/input/breast-histopathology-images/9176/1/9176_idx5_x2201_y1201_class1.png'  # e.g. https://notebooks.azure.com/clewolff/libraries/otsu/raw/golf1.jpg\nimage = cv2.imread(image_path, cv2.IMREAD_COLOR)\nmask_simple = compute_simple_mask(image)\n\n\ndef show_mask(mask, image, title='', mask_color=(255, 0, 0)):\n    display_image = image.copy()\n    display_image[mask != 0] = mask_color\n    plt.imshow(display_image)\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n\n\nshow_mask(mask_simple, image, title='Simple grayscale thresholding')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_intensity_histogram(image):\n    image_grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    plt.hist(image_grayscale.ravel(), 256)\n    plt.title('Intensity histogram')\n    plt.ylabel('Number of pixels')\n    plt.xlabel('Light intensity')\n    plt.show()\n\n\nshow_intensity_histogram(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_otsu_mask(image):\n  image_grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n  return cv2.threshold(image_grayscale, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\nimage_path = '/kaggle/input/breast-histopathology-images/9176/1/9176_idx5_x2201_y1201_class1.png'  # e.g. https://notebooks.azure.com/clewolff/libraries/otsu/raw/golf1.jpg\nimage = cv2.imread(image_path)\nmask_otsu = compute_otsu_mask(image)\nshow_mask(mask_otsu, image, title='Otsu grayscale thresholding')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nkernel = np.ones((5, 5), np.uint8)\n\nmask_otsu_clean = cv2.morphologyEx(mask_otsu, cv2.MORPH_OPEN, kernel, iterations=2)\nmask_otsu_clean = cv2.erode(mask_otsu_clean, kernel, iterations=2)\nmask_otsu_clean = cv2.dilate(mask_otsu_clean, kernel, iterations=5)\n\nshow_mask(mask_otsu_clean, image, title='Otsu grayscale thresholding with morphological cleanup')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_otsu_mask_hue(image):\n    image_hls = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)\n\n    hue, lightness, saturation = np.split(image_hls, 3, axis=2)\n\n    hue = hue.reshape((hue.shape[0], hue.shape[1]))\n\n    otsu = cv2.threshold(hue, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n    otsu_mask = otsu != 255\n\n    return otsu_mask\n\n\nmask_otsu_hue = compute_otsu_mask_hue(image)\nshow_mask(mask_otsu_hue, image, title='Otsu thresholding on the hue channel')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_otsu_mask_shadows(image, shadow_percentile=5):\n    image_hls = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)\n\n    hue, lightness, saturation = np.split(image_hls, 3, axis=2)\n    hue = hue.reshape((hue.shape[0], hue.shape[1]))\n\n    otsu = cv2.threshold(hue, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n    otsu_mask = otsu != 255\n\n    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n    l, a, b = np.split(image_lab, 3, axis=2)\n    l = l.reshape((l.shape[0], l.shape[1]))\n\n    shadow_threshold = np.percentile(l.ravel(), q=shadow_percentile)\n    shadows_mask = l < shadow_threshold\n\n    mask = otsu_mask ^ shadows_mask\n\n    return mask\n\n\nimage_path2 = '/kaggle/input/breast-histopathology-images/14304/0/14304_idx5_x1151_y2201_class0.png'  # e.g. https://notebooks.azure.com/clewolff/libraries/otsu/raw/golf2.jpg\nimage2 = cv2.imread(image_path2)\nmask_otsu_shadows = compute_otsu_mask_shadows(image2)\nshow_mask(mask_otsu_shadows, image2, title='Otsu thresholding on the hue channel with shadow removal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes=2\nout = Conv2D(n_classes ,(1, 1) , padding='same')(conv5)\n\nfrom keras_segmentation.models.model_utils import get_segmentation_model\n\nmodel = get_segmentation_model(img_input ,  out ) # this would build the segmentation model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train( \n    train_images =  \"/kaggle/input/breast-histopathology-images/9176/0/9176_idx5_x2101_y151_class0.png\",\n    train_annotations = \"/kaggle/input/breast-histopathology-images/9176/0\",\n    checkpoints_path = \"checkpoints/vgg_unet_1\" , epochs=5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pillow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport cv2\n#from scipy.misc import imresize, imread\nfrom PIL import Image\nimport numpy as np\nimport sklearn\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport keras\nfrom keras import backend as K\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, model_from_json\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\nfrom keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nlistdir('/kaggle/input')[:10]\ndef proc_images():\n    \"\"\"\n    Returns two arrays: \n        x is an array of resized images\n        y is an array of labels\n    \"\"\" \n    x = []\n    y = []\n    WIDTH = 50\n    HEIGHT = 50\n    for filename in range(len(image_files)):\n        print(image_files[filename])\n        img=image_files[filename]\n        full_size_image = cv2.imread(image_files[filename])\n        grayimg = cv2.cvtColor(full_size_image, cv2.COLOR_BGR2GRAY)\n        median = cv2.medianBlur(grayimg,5)\n        x.append(cv2.resize(median, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n        \n        if img in classZero:\n            \n            y.append(0)\n        elif img in classOne:\n            y.append(1)\n        else:\n            return\n    return x,y\nX,Y = proc_images()\nX1 = np.array(X)\ndf = pd.DataFrame()\ndf[\"images\"]=X\ndf[\"labels\"]=Y\nlen(image_files)\n\nX2=df[\"images\"]\nY2=df[\"labels\"]\n\ntype(X2)\n\nX2=np.array(X2)\n\nX2.shape\n\nimgs0=[]\nimgs1=[]\nimgs0 = X2[Y2==0] # (0 = no IDC, 1 = IDC)\nimgs1 = X2[Y2==1] \n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def describeData(a,b):\n    print('Total number of images: {}'.format(len(a)))\n    print('Number of IDC(-) Images: {}'.format(np.sum(b==0)))\n    print('Number of IDC(+) Images: {}'.format(np.sum(b==1)))\n    print('Percentage of positive images: {:.2f}%'.format(100*np.mean(b)))\n    print('Image shape (Width, Height, Channels): {}'.format(a[0].shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_classes_split(series):\n    ratio = np.round(series.value_counts()/series.count()*100,decimals = 1)\n    return ratio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(get_classes_split(data['target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Set the patient ids as indices for the dataframe\nshuffled_data = data.set_index('patient_id')\n# Select all columns except the target column and store it in X\nX = shuffled_data.loc[:, shuffled_data.columns != 'target']\n# Select the target column and store it in y \ny = shuffled_data['target']\n\n# OS stands for 'Out of Sample'\nX_data, OSX_df, y_data, OSy_df = train_test_split(X, y, test_size=0.1, shuffle = False)\n# We split it even further and obtain the training and testing dataframes\nX_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_data, y_data, test_size=0.3, shuffle = False)\n\n#Let's ensure that our classes split is maintained after the shuffling and splitting.\n\nprint(f'y_data: {get_classes_split(y_data)}')\ndisplay(get_classes_split(y_test_df))\ndisplay(get_classes_split(OSy_df))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualise_breast_tissue(patient_id, df = data,pred = False, crop_dimension = [50,50]):\n    # Plotting Settings\n    plt.xticks([])\n    plt.yticks([])\n    # Get patient dataframe\n    p_df = get_patient_df(patient_id)\n    # Get the dimensions of the breast tissue image\n    max_coord = np.max((*p_df.x,*p_df.y))\n    # Allocate an array to fill image pixels in,use uint8 type as you don't need an int over 255\n    grid = 255*np.ones(shape = (max_coord + crop_dimension[0], max_coord + crop_dimension[1], 3)).astype(np.uint8)\n    mask = 255*np.ones(shape = (max_coord + crop_dimension[0], max_coord + crop_dimension[1], 3)).astype(np.uint8)\n    # Replace array values with values of the image\n    for x,y,target,path in zip(p_df['x'],p_df['y'],p_df['target'],p_df['path']):\n        try:\n            img = io.imread(path)\n            # Replace array values with cropped image values\n            grid[y:y+crop_dimension[1],x:x+crop_dimension[0]] = img\n            # Check if target is cancerous or not\n            if target != 0:\n                # If the target is cancerous then, replace array values with the color blue\n                mask[y:y+crop_dimension[1],x:x+crop_dimension[0]] = [0,0,255]\n        except: pass\n    # if prediction is not specifies then show the image normally\n    if pred == False:\n        io.imshow(grid)\n        img = grid\n    # if prediction is specified then apply a mask to the areas that contain predicted cancerous cells\n    else:\n        # Specify the desired alpha value\n        alpha = 0.78\n        # This is step is very important, adding 2 numpy arrays sets the values to float64, which is why convert them back to uint8\n        img = (mask * (1.0 - alpha) + grid * alpha).astype('uint8')\n        io.imshow(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        p_id = patient_ids[col + n_cols*row]\n        \n        img = visualise_breast_tissue(p_id, pred = True)\n        ax[row,col].grid(False)\n        ax[row,col].set_xticks([])\n        ax[row,col].set_yticks([])\n        ax[row,col].set_title(\"Breast tissue slice of patient: \" + p_id)        \n        ax[row,col].imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe containing cancerous images\nc_df = data.loc[data.target == 1,:]\n# Dataframe containing normal images\nn_df = data.loc[data.target == 0,:]\n\nfraction_c = np.round(0.7*c_df.shape[0]).astype(int)\nfraction_n = np.round(0.2*n_df.shape[0]).astype(int)\n\nrest_c_df = c_df.iloc[fraction_c:-1]\nrest_n_df = n_df.iloc[fraction_n:-1]\n\nc_df = c_df.iloc[0: fraction_c]\nn_df = n_df.iloc[0: fraction_n]\n\nnc = c_df.shape[0]\nnn = n_df.shape[0]\n\nnrc = rest_c_df.shape[0]\nnrn = rest_n_df.shape[0]\n\ntotal_test = nn+nc\ntotal_train = nrn + nrc\n\nprint(\"Testing Data:\") \nprint(f'percent cancerous : {round(nc/total_test*100,1)}%')\nprint(f'percent non-cancerous : {round(nn/total_test*100,1)}%\\n')\nprint(\"Training Data:\")\nprint(f'percent cancerous : {round(nrc/total_train*100,1)}%')\nprint(f'percent non-cancerous : {round(nrn/total_train*100,1)}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_train_df = c_df.append(n_df, sort = True).reset_index(drop=True)\nn_test_df = rest_c_df.append(rest_n_df,sort = True).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nfrom sklearn.decomposition import IncrementalPCA\ndef rgb_to_grayscale(img_paths, batch_size = 15000):\n    # get the total number of images\n    num_of_imgs = img_paths.shape[0]\n    # initialize counter that keeps track of position of image being loaded\n    pos = 0\n    # initialize empty array in order fill in the image values\n    grid = np.zeros((num_of_imgs*2500, 3))\n\n    for img_path in tqdm(img_paths, total=num_of_imgs):\n        # Read the image into a numpy array \n        img = io.imread(img_path)\n        # reshape the image to such that the rgb values are the columns of the matrix\n        img = img.reshape(-1, 3)\n        # replace the empty array with the values inside the image\n        grid[pos: pos + img.shape[0],:] = img\n        # update position counter\n        pos += img.shape[0]\n        \n    # initialize pca to reduce rgb scale to a single dimensional scale\n    ipca = IncrementalPCA(n_components=1, batch_size=batch_size)\n    # fit pca object to the contents within the grid\n    ipca.fit(grid)\n    # delete grid to free up sum memory\n    del grid\n    gc.collect()\n    \n    return ipca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_paths = n_train_df['path']\nrgb_pca = rgb_to_grayscale(img_paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rgb_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NEW CODING WITH CNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imagePatches = image_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patternZero = '*class0.png'\npatternOne = '*class1.png'\nclassZero = fnmatch.filter(imagePatches, patternZero)\nclassOne = fnmatch.filter(imagePatches, patternOne)\nprint(\"IDC(-)\\n\\n\",classZero[0:5],'\\n')\nprint(\"IDC(+)\\n\\n\",classOne[0:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def proc_images(lowerIndex,upperIndex):\n    \"\"\"\n    Returns two arrays: \n        x is an array of resized images\n        y is an array of labels\n    \"\"\" \n    x = []\n    y = []\n    WIDTH = 50\n    HEIGHT = 50\n    for img in imagePatches[lowerIndex:upperIndex]:\n        full_size_image = cv2.imread(img)\n        #grayimg = cv2.cvtColor(full_size_image, cv2.COLOR_BGR2GRAY)\n        median = cv2.medianBlur(full_size_image,5)\n        x.append(cv2.resize(median, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n        if img in classZero:\n            y.append(0)\n        elif img in classOne:\n            y.append(1)\n        else:\n            return\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y = proc_images(0,5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = np.array(X)\n\nX1.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\ndf[\"images\"]=X\ndf[\"labels\"]=Y\n\nX2=df[\"images\"]\nY2=df[\"labels\"]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(X2)\n\n\n\nX2=np.array(X2)\n\nX2.shape\n\n\n\nimgs0=[]\nimgs1=[]\nimgs0 = X2[Y2==0] # (0 = no IDC, 1 = IDC)\nimgs1 = X2[Y2==1] \n\ndef describeData(a,b):\n    print('Total number of images: {}'.format(len(a)))\n    print('Number of IDC(-) Images: {}'.format(np.sum(b==0)))\n    print('Number of IDC(+) Images: {}'.format(np.sum(b==1)))\n    print('Percentage of positive images: {:.2f}%'.format(100*np.mean(b)))\n    print('Image shape (Width, Height, Channels): {}'.format(a[0].shape))\ndescribeData(X2,Y2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotTwo(a,b): \n    \"\"\"\n    Plot a bunch of numpy arrays sorted by label\n    \"\"\"\n    for row in range(3):\n        plt.figure(figsize=(20, 10))\n        for col in range(3):\n            plt.subplot(1,8,col+1)\n            plt.title('IDC (-)')\n            plt.imshow(a[0+row+col])\n            plt.axis('off')       \n            plt.subplot(1,8,col+4)\n            plt.title('IDC (+)')\n            plt.imshow(b[0+row+col])\n            plt.axis('off')\nplotTwo(imgs0, imgs1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotHistogram(a):\n    \"\"\"\n    Plot histogram of RGB Pixel Intensities\n    \"\"\"\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plt.imshow(a)\n    plt.axis('off')\n    plt.title('IDC(+)' if Y[1] else 'IDC(-)')\n    histo = plt.subplot(1,2,2)\n    histo.set_ylabel('Count')\n    histo.set_xlabel('Pixel Intensity')\n    n_bins = 30\n    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);\nplotHistogram(X2[100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=np.array(X)\nX=X/255.0\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\nprint(df.head(10))\nprint(\"\")\nprint(dict_characters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist = df['labels'].value_counts()\nlab = df['labels']\n\ndist\n\n\n\nsns.countplot(df['labels'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nfrom keras.utils.np_utils import to_categorical\ny_trainHot = to_categorical(Y_train, num_classes = 2)\ny_testHot = to_categorical(Y_test, num_classes = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import class_weight\nclass_weight = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\nprint(\"Old Class Weights: \",class_weight)\nfrom sklearn.utils import class_weight\nclass_weight2 = class_weight.compute_class_weight('balanced', np.unique(Y_trainRos), Y_trainRos)\nprint(\"New Class Weights: \",class_weight2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#comsume more memory don't run\n# Deal with imbalanced class sizes below\n# Make Data 1D for compatability upsampling methods\nX_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\nX_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\nX_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\nX_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\n#print(\"X_train Shape: \",X_train.shape)\n#print(\"X_test Shape: \",X_test.shape)\n#print(\"X_trainFlat Shape: \",X_trainFlat.shape)\n#print(\"X_testFlat Shape: \",X_testFlat.shape)\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n#ros = RandomOverSampler(ratio='auto')\nros = RandomUnderSampler(sampling_strategy='auto')\nX_trainRos, Y_trainRos = ros.fit_sample(X_trainFlat, Y_train)\nX_testRos, Y_testRos = ros.fit_sample(X_testFlat, Y_test)\n\n# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_trainRosHot = to_categorical(Y_trainRos, num_classes = 2)\nY_testRosHot = to_categorical(Y_testRos, num_classes = 2)\n#print(\"X_train: \", X_train.shape)\n#print(\"X_trainFlat: \", X_trainFlat.shape)\n#print(\"X_trainRos Shape: \",X_trainRos.shape)\n#print(\"X_testRos Shape: \",X_testRos.shape)\n#print(\"Y_trainRosHot Shape: \",Y_trainRosHot.shape)\n#print(\"Y_testRosHot Shape: \",Y_testRosHot.shape)\n\nfor i in range(len(X_trainRos)):\n    height, width, channels = 50,50,3\n    X_trainRosReshaped = X_trainRos.reshape(len(X_trainRos),height,width,channels)\n#print(\"X_trainRos Shape: \",X_trainRos.shape)\n#print(\"X_trainRosReshaped Shape: \",X_trainRosReshaped.shape)\n\nfor i in range(len(X_testRos)):\n    height, width, channels = 50,50,3\n    X_testRosReshaped = X_testRos.reshape(len(X_testRos),height,width,channels)\n#print(\"X_testRos Shape: \",X_testRos.shape)\n#print(\"X_testRosReshaped Shape: \",X_testRosReshaped.shape)\n\ndfRos = pd.DataFrame()\ndfRos[\"labels\"]=Y_trainRos\nlabRos = dfRos['labels']\ndistRos = lab.value_counts()\nsns.countplot(labRos)\nprint(dict_characters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MetricsCheckpoint(Callback):\n    \"\"\"Callback that saves metrics after each epoch\"\"\"\n    def __init__(self, savepath):\n        super(MetricsCheckpoint, self).__init__()\n        self.savepath = savepath\n        self.history = {}\n    def on_epoch_end(self, epoch, logs=None):\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        np.save(self.savepath, self.history)\n\ndef plotKerasLearningCurve():\n    plt.figure(figsize=(10,5))\n    metrics = np.load('logs.npy',allow_pickle=True)[()]\n    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n        l = np.array(metrics[k])\n        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n        y = l[x]\n        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n    plt.legend(loc=4)\n    plt.axis([0, None, None, None]);\n    plt.grid()\n    plt.xlabel('Number of epochs')\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (5,5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    total=sum(sum(cm))\n    accuracy=(cm[0,0]+cm[1,1])/total\n    specificity=cm[1,1]/(cm[1,0]+cm[1,1])\n    sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n    print(cm)\n    print(f'Accuracy: {accuracy}')\n    print(f'Specificity: {specificity}')\n    print(f'Sensitivity: {sensitivity}')\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n\ndef plot_learning_curve(history):\n    plt.figure(figsize=(8,8))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig('./accuracy_curve.png')\n    #plt.clf()\n    # summarize history for loss\n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig('./loss_curve.png')\n\nbatch_size = 128\nnum_classes = 2\nepochs = 8\nimg_rows,img_cols=50,50\ninput_shape = (img_rows, img_cols, 3)\ne = 2\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape,strides=e))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\ncnn=model.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True)  # randomly flip images\n\na = X_train\nb = y_trainHot\nc = X_test\nd = y_testHot\nepochs = 10\n\nhistory = model.fit_generator(datagen.flow(a,b, batch_size=32),\n                        steps_per_epoch=len(a)/2, \n                              epochs=epochs,validation_data = [c, d],\n                              callbacks = [MetricsCheckpoint('logs')])\n\ny_pred = model.predict(c)\n\nY_pred_classes = np.argmax(y_pred,axis=1) \nY_true = np.argmax(d,axis=1)\n\ndict_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \nplot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) \nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def runKerasCNNAugment(a,b,c,d,e,f):\n    \"\"\"\n    Run Keras CNN: https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n    \"\"\"\n    batch_size = 128\n    num_classes = 2\n    epochs = 8\n#     img_rows, img_cols = a.shape[1],a.shape[2]\n    img_rows,img_cols=50,50\n    input_shape = (img_rows, img_cols, 3)\n    model2 = Sequential()\n    model2.add(Conv2D(32, kernel_size=(3, 3),\n                     activation='relu',\n                     input_shape=input_shape,strides=e))\n    model2.add(Conv2D(64, (3, 3), activation='relu'))\n    model2.add(MaxPooling2D(pool_size=(2, 2)))\n    model2.add(Dropout(0.25))\n    model2.add(Flatten())\n    model2.add(Dense(128, activation='relu'))\n    model2.add(Dropout(0.5))\n    model2.add(Dense(num_classes, activation='softmax'))\n    model2.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adadelta(),\n                  metrics=['accuracy'])\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True)  # randomly flip images\n    history = model2.fit_generator(datagen.flow(a,b, batch_size=32),\n                        steps_per_epoch=len(a) / 32, epochs=epochs,class_weight=f, validation_data = [c, d],callbacks = [MetricsCheckpoint('logs')])\n    score = model2.evaluate(c,d, verbose=0)\n    print('\\nKeras CNN #1C - accuracy:', score[1],'\\n')\n    y_pred = model2.predict(c)\n    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')    \n    Y_pred_classes = np.argmax(y_pred,axis=1) \n    Y_true = np.argmax(d,axis=1) \n    plotKerasLearningCurve()\n    plt.show()  \n    plot_learning_curve(history)\n    plt.show()\n    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n    plot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) \n    plt.show()\nrunKerasCNNAugment(X_trainRosReshaped, Y_trainRosHot, X_testRosReshaped, Y_testRosHot,2,class_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate two models, doesn't three\nconcat_a = tf.keras.layers.concatenate([model.output, \n                                      model2.output])\nmodel_a = tf.keras.Model([model.input, model2.input], concat_a)\n\nconcat = tf.keras.layers.concatenate([model_a.output, \n                                        model3.output])\n\ndense = tf.keras.layers.Dense(1024)(concat)\nrelu = tf.keras.layers.LeakyReLU(alpha=0.3)(dense)\nnormalize = tf.keras.layers.BatchNormalization()(relu)\nout = tf.keras.layers.Dense(10, activation='softmax', name='output_layer')(normalize)\n\n# nested list\nmodel = tf.keras.Model([[model_1.input, model_2.input], model_3.input], out)\n\nmodel.summary()\n\noptimizer = RMSprop()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\n\n# simple list\nhistory = model.fit([trainX, trainX, trainX], trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel1 = Sequential()\n#model1.add(Dropout(0.25))\nmodel1.add(Flatten())\nmlp=model1.add(Dense(128, activation='relu', input_shape=input_shape))\n#model.add(Dropout(0.2))\nmodel.add(Dense(128, activation='relu'))\nmodel1.add(Dense(num_classes, activation='softmax'))\nmodel1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory1 = model1.fit(a, b, batch_size=32, epochs=10, verbose=1,validation_data=(c,d))\nmodel1.summary()\n\n                            \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MLP\ntest_scores = model1.evaluate(c,d, verbose=2)\nprint('Test loss:', test_scores[0])\nprint('Test accuracy:', test_scores[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import concatenate\nfrom keras.layers import PReLU\n\nmerged_layers = concatenate([model.output, model1.output])\nx = BatchNormalization()(merged_layers)\nx = Dense(128)(x)\nx = PReLU()(x)\nx = Dropout(0.2)(x)\nx = Dense(1)(x)\nx = BatchNormalization()(x)\n\nout1 = Activation('sigmoid')(x)\nout2 = Activation('sigmoid')(x)\nmerged_model = Model([model.input, model1.input], [out1,out2])\n\nmerged_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n#merged_model.metrics.accuracy(c,d)\n#history2 = merged_model.fit(a, b, batch_size=32, epochs=10, verbose=1,validation_data=(c,d))\n#merged_model.fit(model,out1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [np.zeros((1,50,50,1))] * 2\ny = np.ones((1,1))\nmerged_model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#working good","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dropout\ninput_shape = (img_rows, img_cols, 3)\n\nmodel_reg = Sequential()\nmodel_reg.add(Dense(512, activation='relu', input_shape=input_shape))\n\nmodel_reg.add(Flatten())\nmodel_reg.add(Dropout(0.5))\n\nmodel_reg.add(Dense(num_classes, activation='softmax'))\nmodel_reg.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory_reg = model_reg.fit(a,b, batch_size=32, epochs=20, verbose=1,validation_data=(c,d))\n\n#Plot the Loss Curves\nplt.figure(figsize=[8,6])\nplt.plot(history_reg.history['loss'],'r',linewidth=3.0)\nplt.plot(history_reg.history['val_loss'],'b',linewidth=3.0)\nplt.legend(['Training loss', 'Validation Loss'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n\n#Plot the Accuracy Curves\nplt.figure(figsize=[8,6])\nplt.plot(history_reg.history['accuracy'],'r',linewidth=3.0)\nplt.plot(history_reg.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_reg.predict_classes(c[[0],:])\nmodel_reg.predict(c[[0],:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#more dense CNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n\nseq_length = 64\n\nmodel = Sequential()\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n#model.fit(a,b, batch_size=16, epochs=10)\n#score = model.evaluate(c,d, batch_size=16)\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True)  # randomly flip images\n\na = X_train\nb = y_trainHot\nc = X_test\nd = y_testHot\nepochs = 10\n\nhistory = model.fit_generator(datagen.flow(a,b, batch_size=32),\n                        steps_per_epoch=len(a)/2, \n                              epochs=epochs,validation_data = [c, d],\n                              callbacks = [MetricsCheckpoint('logs')])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = X_train\nb = y_trainHot\nc = X_test\nd = y_testHot\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = X_train\nb = y_trainHot\nc = X_test\nd = y_testHot\nepochs = 10\n\n\ny_pred = model1.predict(c)\n\nY_pred_classes = np.argmax(y_pred,axis=1) \nY_true = np.argmax(d,axis=1)\n\ndict_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \nplot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) \nplt.show()\ntest_scores = model1.evaluate(c,d, verbose=2)\nprint('Test loss:', test_scores[0])\nprint('Test accuracy:', test_scores[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"model3 = keras.Sequential([\n    keras.layers.Flatten(input_shape=input_shape),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10)\n])\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = keras.Sequential([ keras.layers.Flatten(input_shape=input_shape), keras.layers.Dense(128, activation='relu'), keras.layers.Dense(10) ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmodel3.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel3.fit(a, b, batch_size=32, epochs=10, verbose=1,validation_data=(c,d))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n#merged_model.fit(x,y,epochs=20,batch_size=10)\nmerged_model.fit(a,b, epochs=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import seed\nfrom pandas import read_csv, DataFrame\nfrom sklearn.preprocessing import scale\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers.merge import average\nfrom keras.layers import Input, Dense, Flatten, Reshape, Dropout, SpatialDropout1D\nfrom keras.models import Model\nfrom keras.optimizers import SGD\nfrom keras.utils import plot_model\n\n\nD = 0.2\nS = 1\nimg_rows,img_cols=50,50\ninput_shape = (img_rows, img_cols, 3)\n\nseed(S)\n### INPUT DATA\ninputs = input_shape\n### DEFINE A MULTILAYER PERCEPTRON NETWORK\n\nmlp_net = Dense(32, activation = 'relu', kernel_initializer = 'he_uniform')(inputs)\nmlp_net = Dropout(rate = D, seed = S)(mlp_net)\nmodel.add(Flatten())\nmlp_net = Dense(32, activation = 'relu', kernel_initializer = 'he_uniform')(mlp_net)\nmlp_net = Dropout(rate = D, seed = S)(mlp_net)\nmodel.add(Flatten())\nmlp_out = Dense(num_classes, activation = 'sigmoid')(mlp_net)\n\n\nmlp_mdl = Model(inputs = inputs, outputs = mlp_out)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### DEFINE A CONVOLUTIONAL NETWORK \ncnv_net = Reshape((a.shape[1], 1))(inputs)\ncnv_net = Conv1D(128, 4, activation = 'relu', padding = \"same\", kernel_initializer = 'he_uniform')(cnv_net)\ncnv_net = MaxPooling1D(2)(cnv_net)\ncnv_net = SpatialDropout1D(D)(cnv_net)\ncnv_net = Flatten()(cnv_net)\ncnv_out = Dense(1, activation = 'sigmoid')(cnv_net)\ncnv_mdl = Model(inputs = inputs, outputs = cnv_out)\n### COMBINE MLP AND CNV\ncon_out = concatenate()([mlp_out, cnv_out])\ncon_mdl = Model(inputs = inputs, outputs = con_out)\nsgd = SGD(lr = 0.1, momentum = 0.9)\ncon_mdl.compile(optimizer = sgd, loss = 'binary_crossentropy',  metrics = ['binary_accuracy'])\ncon_mdl.fit(a, b, batch_size = 2000, epochs = 50, verbose = 0)\nplot_model(con_mdl, to_file = 'model.png', show_shapes = True, show_layer_names = True)### DEFINE A CONVOLUTIONAL NETWORK \ncnv_net = Reshape((a.shape[1], 1))(inputs)\ncnv_net = Conv1D(128, 4, activation = 'relu', padding = \"same\", kernel_initializer = 'he_uniform')(cnv_net)\ncnv_net = MaxPooling1D(2)(cnv_net)\ncnv_net = SpatialDropout1D(D)(cnv_net)\ncnv_net = Flatten()(cnv_net)\ncnv_out = Dense(1, activation = 'sigmoid')(cnv_net)\ncnv_mdl = Model(inputs = inputs, outputs = cnv_out)\n### COMBINE MLP AND CNV\ncon_out = concatenate()([mlp_out, cnv_out])\ncon_mdl = Model(inputs = inputs, outputs = con_out)\nsgd = SGD(lr = 0.1, momentum = 0.9)\ncon_mdl.compile(optimizer = sgd, loss = 'binary_crossentropy',  metrics = ['binary_accuracy'])\ncon_mdl.fit(a, b, batch_size = 2000, epochs = 50, verbose = 0)\nplot_model(con_mdl, to_file = 'model.png', show_shapes = True, show_layer_names = True)### DEFINE A CONVOLUTIONAL NETWORK \ncnv_net = Reshape((a.shape[1], 1))(inputs)\ncnv_net = Conv1D(128, 4, activation = 'relu', padding = \"same\", kernel_initializer = 'he_uniform')(cnv_net)\ncnv_net = MaxPooling1D(2)(cnv_net)\ncnv_net = SpatialDropout1D(D)(cnv_net)\ncnv_net = Flatten()(cnv_net)\ncnv_out = Dense(1, activation = 'sigmoid')(cnv_net)\ncnv_mdl = Model(inputs = inputs, outputs = cnv_out)\n### COMBINE MLP AND CNV\ncon_out = concatenate()([mlp_out, cnv_out])\ncon_mdl = Model(inputs = inputs, outputs = con_out)\nsgd = SGD(lr = 0.1, momentum = 0.9)\ncon_mdl.compile(optimizer = sgd, loss = 'binary_crossentropy',  metrics = ['binary_accuracy'])\ncon_mdl.fit(a, b, batch_size = 2000, epochs = 50, verbose = 0)\nplot_model(con_mdl, to_file = 'model.png', show_shapes = True, show_layer_names = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def all_cnn(model_input: Tensor) -> training.Model:\n    \n    x = Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same')(model_input)\n    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)\n    x = Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2)(x)\n    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n    x = Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2)(x)\n    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n    x = Conv2D(192, (1, 1), activation='relu')(x)\n    x = Conv2D(10, (1, 1))(x)\n    x = GlobalAveragePooling2D()(x)\n    x = Activation(activation='softmax')(x)\n        \n    model = Model(model_input, x, name='all_cnn')\n    \n    return modelall_cnn_model = all_cnn(model_input)\n_, all_cnn_weight_file = compile_and_train(all_cnn_model, NUM_EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combinedInput = concatenate([history, history1])\n# our final FC layer head will have two dense layers, the final one\n# being our regression head\nx = Dense(4, activation=\"relu\")(combinedInput)\nx = Dense(1, activation=\"linear\")(x)\n# our final model will accept categorical/numerical data on the MLP\n# input and images on the CNN input, outputting a single value (the\n# predicted price of the house)\nmodel = Model(inputs=[mlp.input, cnn.input], outputs=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotKerasLearningCurve()\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curve(history)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = AdaBoostClassifier(model, n_estimators=3, algorithm=\"SAMME.R\", learning_rate=0.5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}